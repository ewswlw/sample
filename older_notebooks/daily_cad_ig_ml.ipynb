{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xbbg import blp\n",
    "import vectorbt as vbt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "from xbbg import blp\n",
    "import os\n",
    "import quantstats as qs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Import custom modules with an alias\n",
    "import bloomberg_data as bd\n",
    "import transformations as tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 12:28:47,897 - INFO - Successfully retrieved data for ticker: .MIDERCAD U Index\n",
      "2024-06-30 12:28:48,551 - INFO - Successfully retrieved data for ticker: .CADIG F Index\n",
      "2024-06-30 12:28:48,923 - INFO - Successfully retrieved data for ticker: VIX Index\n",
      "2024-06-30 12:28:49,459 - INFO - Successfully retrieved data for ticker: .HYUSER U Index\n",
      "2024-06-30 12:28:50,399 - INFO - Successfully retrieved data for ticker: .IGUSER U Index\n",
      "2024-06-30 12:28:50,765 - INFO - Successfully retrieved data for ticker: GCAN3M Index\n",
      "2024-06-30 12:28:50,769 - WARNING - Cannot infer frequency. Attempting to resample to daily frequency.\n",
      "2024-06-30 12:28:50,780 - INFO - Merged 5 dataframes using inner method.\n",
      "2024-06-30 12:28:50,782 - INFO - Merged 2 dataframes using inner method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            cad_ig_er_index  cad_ig_sprds    vix  us_hy_er_index  \\\n",
      "2002-11-29           1.0143       69.8153  27.50          0.4183   \n",
      "2002-12-31           1.0146       77.3398  28.62          0.4134   \n",
      "2003-01-31           1.0155       74.8880  31.17          0.4285   \n",
      "2003-02-28           1.0159      106.9295  29.63          0.4265   \n",
      "2003-03-31           1.0142      117.3892  29.15          0.4406   \n",
      "...                     ...           ...    ...             ...   \n",
      "2024-06-24           1.3922      120.6286  13.33          1.1202   \n",
      "2024-06-25           1.3924      120.6766  12.84          1.1198   \n",
      "2024-06-26           1.3935      120.6997  12.55          1.1236   \n",
      "2024-06-27           1.3933      120.9534  12.24          1.1215   \n",
      "2024-06-28           1.3925      120.2679  12.44          1.1270   \n",
      "\n",
      "            us_ig_er_index  risk_free_index  \n",
      "2002-11-29          1.0150       112.289960  \n",
      "2002-12-31          1.0195       112.557157  \n",
      "2003-01-31          1.0269       112.816244  \n",
      "2003-02-28          1.0303       113.064659  \n",
      "2003-03-31          1.0351       113.364442  \n",
      "...                    ...              ...  \n",
      "2024-06-24          1.4223       164.445511  \n",
      "2024-06-25          1.4211       164.466497  \n",
      "2024-06-26          1.4201       164.487485  \n",
      "2024-06-27          1.4207       164.508468  \n",
      "2024-06-28          1.4199       164.529448  \n",
      "\n",
      "[2033 rows x 6 columns]\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2033 entries, 2002-11-29 to 2024-06-28\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   cad_ig_er_index  2033 non-null   float64\n",
      " 1   cad_ig_sprds     2033 non-null   float64\n",
      " 2   vix              2033 non-null   float64\n",
      " 3   us_hy_er_index   2033 non-null   float64\n",
      " 4   us_ig_er_index   2033 non-null   float64\n",
      " 5   risk_free_index  2033 non-null   float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 111.2 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Getting the data from the bloomberg_data module \n",
    "tickers = ['.MIDERCAD U Index', '.CADIG F Index', 'VIX Index','.HYUSER U Index','.IGUSER U Index']\n",
    "fields = [['PX_LAST'], ['PX_LAST'], ['PX_LAST'],['PX_LAST'], ['PX_LAST']]\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2025-12-31'\n",
    "column_names = [['cad_ig_er_index'], ['cad_ig_sprds'], ['vix'], ['us_hy_er_index'], ['us_ig_er_index']]\n",
    "frequencies = ['D', 'D', 'D','D','D']  # You can edit the frequency for each ticker here\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for ticker, field, col_name, freq in zip(tickers, fields, column_names, frequencies):\n",
    "    df = bd.get_single_ticker_data(ticker, field, start_date, end_date, freq=freq, column_names=col_name)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Getting risk-free index\n",
    "rate_df = bd.get_single_ticker_data('GCAN3M Index', ['PX_LAST'], start_date, end_date)\n",
    "risk_free_idx = tr.risk_free_index(rate_df,col_name=\"risk_free\")  # Ensure the default col_name is applied\n",
    "\n",
    "# Merge all dataframes including the risk-free index\n",
    "merged_data = bd.merge_dataframes(dataframes)\n",
    "merged_data = bd.merge_dataframes([merged_data, risk_free_idx])\n",
    "\n",
    "# Print the final merged data and its information\n",
    "print(merged_data)\n",
    "print('----------------------------------------------------------------')\n",
    "print('----------------------------------------------------------------')\n",
    "print(merged_data.info())\n",
    "\n",
    "# Rename\n",
    "data= merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to Outputs\\daily_credit_ml.csv\n"
     ]
    }
   ],
   "source": [
    "# Reset the index to move the date from the index to a column\n",
    "data_reset = data.reset_index()\n",
    "data_reset.rename(columns={'index': 'Date'}, inplace=True)\n",
    "\n",
    "# Ensure the Outputs directory exists\n",
    "output_dir = 'Outputs'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the DataFrame to a CSV file in the Outputs folder\n",
    "output_file_path = os.path.join(output_dir, 'daily_credit_ml.csv')\n",
    "data_reset.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2033 entries, 2002-11-29 to 2024-06-28\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   cad_ig_er_index  2033 non-null   float64\n",
      " 1   cad_ig_sprds     2033 non-null   float64\n",
      " 2   vix              2033 non-null   float64\n",
      " 3   us_hy_er_index   2033 non-null   float64\n",
      " 4   us_ig_er_index   2033 non-null   float64\n",
      " 5   risk_free_index  2033 non-null   float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 111.2 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal sum: 226\n",
      "Signal mean: 0.11116576487948844\n",
      "Number of trade entries: 39\n",
      "\n",
      "Portfolio Statistics:\n",
      "Start                         2002-11-29 00:00:00\n",
      "End                           2024-06-28 00:00:00\n",
      "Period                         2033 days 00:00:00\n",
      "Start Value                              100000.0\n",
      "End Value                           133714.230843\n",
      "Total Return [%]                        33.714231\n",
      "Benchmark Return [%]                    37.286799\n",
      "Max Gross Exposure [%]                      100.0\n",
      "Total Fees Paid                               0.0\n",
      "Max Drawdown [%]                        20.853258\n",
      "Max Drawdown Duration           296 days 00:00:00\n",
      "Total Trades                                    1\n",
      "Total Closed Trades                             0\n",
      "Total Open Trades                               1\n",
      "Open Trade PnL                       33714.230843\n",
      "Win Rate [%]                                  NaN\n",
      "Best Trade [%]                                NaN\n",
      "Worst Trade [%]                               NaN\n",
      "Avg Winning Trade [%]                         NaN\n",
      "Avg Losing Trade [%]                          NaN\n",
      "Avg Winning Trade Duration                    NaT\n",
      "Avg Losing Trade Duration                     NaT\n",
      "Profit Factor                                 NaN\n",
      "Expectancy                                    NaN\n",
      "Sharpe Ratio                             0.793991\n",
      "Calmar Ratio                             0.256777\n",
      "Omega Ratio                              1.202935\n",
      "Sortino Ratio                            1.149885\n",
      "dtype: object\n",
      "\n",
      "Buy-and-Hold Benchmark Statistics:\n",
      "Start                         2002-11-29 00:00:00\n",
      "End                           2024-06-28 00:00:00\n",
      "Period                         2033 days 00:00:00\n",
      "Start Value                              100000.0\n",
      "End Value                           137286.798777\n",
      "Total Return [%]                        37.286799\n",
      "Benchmark Return [%]                    37.286799\n",
      "Max Gross Exposure [%]                      100.0\n",
      "Total Fees Paid                               0.0\n",
      "Max Drawdown [%]                        20.853258\n",
      "Max Drawdown Duration           296 days 00:00:00\n",
      "Total Trades                                    1\n",
      "Total Closed Trades                             0\n",
      "Total Open Trades                               1\n",
      "Open Trade PnL                       37286.798777\n",
      "Win Rate [%]                                  NaN\n",
      "Best Trade [%]                                NaN\n",
      "Worst Trade [%]                               NaN\n",
      "Avg Winning Trade [%]                         NaN\n",
      "Avg Losing Trade [%]                          NaN\n",
      "Avg Winning Trade Duration                    NaT\n",
      "Avg Losing Trade Duration                     NaT\n",
      "Profit Factor                                 NaN\n",
      "Expectancy                                    NaN\n",
      "Sharpe Ratio                             0.861071\n",
      "Calmar Ratio                              0.28075\n",
      "Omega Ratio                              1.219692\n",
      "Sortino Ratio                            1.249249\n",
      "Name: cad_ig_er_index, dtype: object\n",
      "\n",
      "Initial Strategy Date Range:\n",
      "Start Date: 2002-11-29 00:00:00\n",
      "End Date: 2024-06-28 00:00:00\n",
      "\n",
      "Benchmark Date Range:\n",
      "Start Date: 2002-11-29 00:00:00\n",
      "End Date: 2024-06-28 00:00:00\n",
      "\n",
      "Original Data Date Range:\n",
      "Start Date: 2002-11-29 00:00:00\n",
      "End Date: 2024-06-28 00:00:00\n",
      "\n",
      "All strategies and data cover the same date range.\n",
      "\n",
      "Total number of trading days: 2033\n",
      "\n",
      "Strategy Comparison:\n",
      "Strategy        Total Return    Sharpe Ratio    Max Drawdown    Start Date           End Date             Variable Name  \n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Initial                   33.71            0.79           20.85 2002-11-29           2024-06-28           portfolio      \n",
      "Benchmark                 37.29            0.86           20.85 2002-11-29           2024-06-28           benchmark      \n",
      "\n",
      "All strategies cover the same date range.\n",
      "\n",
      "Total number of trading days: 2033\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vectorbt as vbt\n",
    "\n",
    "# 1. Data Preparation\n",
    "close_data = data[['cad_ig_er_index', 'us_ig_er_index', 'us_hy_er_index', 'risk_free_index']]\n",
    "close_data.index = pd.to_datetime(close_data.index)\n",
    "close_data = close_data.sort_index()\n",
    "\n",
    "# 2. Signal Generation (Modified to use boolean operations)\n",
    "def generate_signal(close, lookback, return_threshold, vol_threshold):\n",
    "    us_ig_returns = close['us_ig_er_index'].pct_change(lookback)\n",
    "    us_hy_returns = close['us_hy_er_index'].pct_change(lookback)\n",
    "    \n",
    "    # Calculate 20-period lookback volatility\n",
    "    us_ig_vol = close['us_ig_er_index'].pct_change().rolling(lookback).std()\n",
    "    us_hy_vol = close['us_hy_er_index'].pct_change().rolling(lookback).std()\n",
    "    \n",
    "    # Generate signal based on returns and volatility\n",
    "    signal = ((us_ig_returns > return_threshold) & (us_hy_returns > return_threshold) & \n",
    "              (us_ig_vol < vol_threshold) & (us_hy_vol < vol_threshold))\n",
    "    \n",
    "    # Ensure the signal is boolean\n",
    "    signal = signal.fillna(False)\n",
    "    \n",
    "    # Debug: Print signal statistics\n",
    "    print(f\"Signal sum: {signal.sum()}\")\n",
    "    print(f\"Signal mean: {signal.mean()}\")\n",
    "    print(f\"Number of trade entries: {(signal.astype(int).diff() == 1).sum()}\")\n",
    "    \n",
    "    return signal\n",
    "\n",
    "# 3. Strategy Implementation\n",
    "lookback = 20\n",
    "return_threshold = 0.01\n",
    "vol_threshold = 0.02  # Example threshold for volatility, you can adjust this\n",
    "\n",
    "signal = generate_signal(close_data, lookback, return_threshold, vol_threshold)\n",
    "\n",
    "# Ensure signal is boolean\n",
    "signal = signal.astype(bool)\n",
    "\n",
    "# 4. Portfolio Creation (Modified to use boolean operations)\n",
    "portfolio = vbt.Portfolio.from_signals(\n",
    "    close=close_data['cad_ig_er_index'],\n",
    "    entries=signal.shift(1).fillna(False),  # Enter the day after the signal\n",
    "    exits=signal.shift(1).fillna(False) & (~signal).fillna(False),  # Exit when signal changes from True to False\n",
    "    init_cash=100000,\n",
    "    fees=0,\n",
    "    freq='D'\n",
    ")\n",
    "\n",
    "# 5. Generate and print portfolio stats\n",
    "try:\n",
    "    stats = portfolio.stats()\n",
    "    print(\"\\nPortfolio Statistics:\")\n",
    "    print(stats)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while generating portfolio stats: {str(e)}\")\n",
    "\n",
    "# 6. Benchmark Creation (Buy-and-Hold)\n",
    "benchmark = vbt.Portfolio.from_holding(\n",
    "    close_data['cad_ig_er_index'],\n",
    "    init_cash=100000,\n",
    "    freq='D'\n",
    ")\n",
    "\n",
    "# 7. Generate and print benchmark stats\n",
    "try:\n",
    "    benchmark_stats = benchmark.stats()\n",
    "    print(\"\\nBuy-and-Hold Benchmark Statistics:\")\n",
    "    print(benchmark_stats)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while generating benchmark stats: {str(e)}\")\n",
    "\n",
    "# Print start and end dates for the initial strategy\n",
    "print(\"\\nInitial Strategy Date Range:\")\n",
    "print(f\"Start Date: {portfolio.wrapper.index[0]}\")\n",
    "print(f\"End Date: {portfolio.wrapper.index[-1]}\")\n",
    "\n",
    "# Print start and end dates for the benchmark\n",
    "print(\"\\nBenchmark Date Range:\")\n",
    "print(f\"Start Date: {benchmark.wrapper.index[0]}\")\n",
    "print(f\"End Date: {benchmark.wrapper.index[-1]}\")\n",
    "\n",
    "# Print the date range of the original data\n",
    "print(\"\\nOriginal Data Date Range:\")\n",
    "print(f\"Start Date: {close_data.index[0]}\")\n",
    "print(f\"End Date: {close_data.index[-1]}\")\n",
    "\n",
    "# Check if all strategies cover the same date range\n",
    "if (portfolio.wrapper.index[0] == benchmark.wrapper.index[0] == close_data.index[0] and\n",
    "    portfolio.wrapper.index[-1] == benchmark.wrapper.index[-1] == close_data.index[-1]):\n",
    "    print(\"\\nAll strategies and data cover the same date range.\")\n",
    "else:\n",
    "    print(\"\\nWarning: Date ranges may not be consistent across all strategies and data.\")\n",
    "    \n",
    "# Print the total number of trading days\n",
    "total_days = len(close_data)\n",
    "print(f\"\\nTotal number of trading days: {total_days}\")\n",
    "\n",
    "# Improved Strategy Comparison Print\n",
    "print(\"\\nStrategy Comparison:\")\n",
    "print(f\"{'Strategy':<15} {'Total Return':<15} {'Sharpe Ratio':<15} {'Max Drawdown':<15} {'Start Date':<20} {'End Date':<20} {'Variable Name':<15}\")\n",
    "print(\"-\" * 115)\n",
    "\n",
    "# Function to format date\n",
    "def format_date(date):\n",
    "    return date.strftime('%Y-%m-%d')\n",
    "\n",
    "# Assuming you have stats and benchmark_stats from previous calculations\n",
    "print(f\"{'Initial':<15} {stats['Total Return [%]']:15.2f} {stats.get('Sharpe Ratio', 'N/A'):15.2f} {stats['Max Drawdown [%]']:15.2f} {format_date(portfolio.wrapper.index[0]):<20} {format_date(portfolio.wrapper.index[-1]):<20} {'portfolio':<15}\")\n",
    "print(f\"{'Benchmark':<15} {benchmark_stats['Total Return [%]']:15.2f} {benchmark_stats.get('Sharpe Ratio', 'N/A'):15.2f} {benchmark_stats['Max Drawdown [%]']:15.2f} {format_date(benchmark.wrapper.index[0]):<20} {format_date(benchmark.wrapper.index[-1]):<20} {'benchmark':<15}\")\n",
    "\n",
    "# Print date range check\n",
    "if (portfolio.wrapper.index[0] == benchmark.wrapper.index[0] and\n",
    "    portfolio.wrapper.index[-1] == benchmark.wrapper.index[-1]):\n",
    "    print(\"\\nAll strategies cover the same date range.\")\n",
    "else:\n",
    "    print(\"\\nWarning: Date ranges are not consistent across all strategies.\")\n",
    "\n",
    "# Print total number of trading days\n",
    "total_days = len(close_data)\n",
    "print(f\"\\nTotal number of trading days: {total_days}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.6823821339950372\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.46      0.42        99\n",
      "           1       0.81      0.75      0.78       304\n",
      "\n",
      "    accuracy                           0.68       403\n",
      "   macro avg       0.60      0.61      0.60       403\n",
      "weighted avg       0.71      0.68      0.69       403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ML Methods\n",
    "\n",
    "# 1. Data Preparation\n",
    "close_data = data[['cad_ig_er_index', 'us_ig_er_index', 'us_hy_er_index', 'risk_free_index']]\n",
    "close_data.index = pd.to_datetime(close_data.index)\n",
    "close_data = close_data.sort_index()\n",
    "\n",
    "# Feature Engineering\n",
    "def create_features(close_data, lookback):\n",
    "    features = pd.DataFrame(index=close_data.index)\n",
    "    features['us_ig_returns'] = close_data['us_ig_er_index'].pct_change(lookback)\n",
    "    features['us_hy_returns'] = close_data['us_hy_er_index'].pct_change(lookback)\n",
    "    features['us_ig_volatility'] = close_data['us_ig_er_index'].pct_change().rolling(window=lookback).std()\n",
    "    features['us_hy_volatility'] = close_data['us_hy_er_index'].pct_change().rolling(window=lookback).std()\n",
    "    features['target'] = (close_data['cad_ig_er_index'].pct_change(lookback).shift(-lookback) > 0).astype(int)\n",
    "    features = features.dropna()\n",
    "    return features\n",
    "\n",
    "lookback = 20\n",
    "features = create_features(close_data, lookback)\n",
    "\n",
    "# Data Preprocessing\n",
    "X = features.drop(columns=['target'])\n",
    "y = features['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Model Training\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Testing Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Signal Generation\n",
    "signals = model.predict(X)\n",
    "features['signal'] = signals\n",
    "\n",
    "# Align signals with the original data\n",
    "signals_series = pd.Series(index=features.index, data=features['signal'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Summary\n",
    "\n",
    "### Training Accuracy\n",
    "- **Training Accuracy: 1.0**: This indicates that the model perfectly predicted the target variable on the training dataset. While this may seem ideal, it is often a sign of overfitting, meaning the model has learned the training data too well, including noise and outliers, which may not generalize well to new, unseen data.\n",
    "\n",
    "### Testing Accuracy\n",
    "- **Testing Accuracy: 0.6824**: This shows that the model correctly predicted the target variable approximately 68.2% of the time on the testing dataset. This performance is significantly lower than the training accuracy, reinforcing the possibility that the model has overfitted to the training data.\n",
    "\n",
    "### Classification Report\n",
    "The classification report provides a detailed breakdown of the model's performance for each class (0 and 1) on the testing dataset. The key metrics include precision, recall, f1-score, and support.\n",
    "\n",
    "#### Class 0 (No Signal)\n",
    "- **Precision: 0.38**: Of all the instances the model predicted as class 0, only 38% were actually class 0. This low precision indicates a high false positive rate.\n",
    "- **Recall: 0.46**: Of all the actual class 0 instances, the model correctly identified 46%. This means the model missed 54% of the actual class 0 instances (high false negative rate).\n",
    "- **F1-Score: 0.42**: The harmonic mean of precision and recall, indicating a balance between the two. A low f1-score suggests the model struggles to correctly classify class 0.\n",
    "- **Support: 99**: The number of actual instances of class 0 in the testing dataset.\n",
    "\n",
    "#### Class 1 (Signal)\n",
    "- **Precision: 0.81**: Of all the instances the model predicted as class 1, 81% were actually class 1. This high precision indicates a low false positive rate.\n",
    "- **Recall: 0.75**: Of all the actual class 1 instances, the model correctly identified 75%. This means the model missed 25% of the actual class 1 instances.\n",
    "- **F1-Score: 0.78**: The harmonic mean of precision and recall, suggesting the model performs well in identifying class 1.\n",
    "- **Support: 304**: The number of actual instances of class 1 in the testing dataset.\n",
    "\n",
    "#### Overall Metrics\n",
    "- **Accuracy: 0.68**: The overall proportion of correctly classified instances in the testing dataset.\n",
    "- **Macro Average**: The average of precision, recall, and f1-score for both classes, giving equal weight to each class.\n",
    "  - **Precision: 0.60**\n",
    "  - **Recall: 0.61**\n",
    "  - **F1-Score: 0.60**\n",
    "- **Weighted Average**: The average of precision, recall, and f1-score, weighted by the number of instances in each class. This gives more importance to the performance on the larger class (class 1 in this case).\n",
    "  - **Precision: 0.71**\n",
    "  - **Recall: 0.68**\n",
    "  - **F1-Score: 0.69**\n",
    "\n",
    "### Interpretation\n",
    "- **Overfitting**: The model performs perfectly on the training data but considerably worse on the testing data, indicating it has overfitted the training data and may not generalize well.\n",
    "- **Class Imbalance**: The support values show there are more instances of class 1 (304) than class 0 (99) in the testing dataset. This imbalance can affect the performance metrics and might require techniques such as resampling, class weighting, or using specialized algorithms to address it.\n",
    "- **Model Performance**: The model has a relatively high precision and recall for class 1 but performs poorly on class 0. This suggests the model is better at predicting the presence of a signal (class 1) than the absence of it (class 0).\n",
    "\n",
    "### Next Steps\n",
    "To improve the model and address the identified issues:\n",
    "1. **Address Overfitting**: Use techniques like cross-validation, pruning the decision trees, or using fewer estimators.\n",
    "2. **Handle Class Imbalance**: Implement resampling techniques like SMOTE or adjust the class weights in the Random Forest model.\n",
    "3. **Feature Engineering**: Explore additional features that might help the model distinguish between the classes better.\n",
    "4. **Model Tuning**: Perform hyperparameter tuning to find the optimal settings for the Random Forest classifier.\n",
    "5. **Experiment with Other Models**: Try other classification algorithms such as Gradient Boosting, SVM, or neural networks to see if they perform better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Accuracy: 0.9426136363636364\n",
      "Random Forest Testing Accuracy: 0.5732009925558312\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.55      0.53       177\n",
      "           1       0.63      0.59      0.61       226\n",
      "\n",
      "    accuracy                           0.57       403\n",
      "   macro avg       0.57      0.57      0.57       403\n",
      "weighted avg       0.58      0.57      0.57       403\n",
      "\n",
      "Gradient Boosting Training Accuracy: 0.9982954545454545\n",
      "Gradient Boosting Testing Accuracy: 0.5781637717121588\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.55      0.54       177\n",
      "           1       0.63      0.60      0.61       226\n",
      "\n",
      "    accuracy                           0.58       403\n",
      "   macro avg       0.57      0.58      0.57       403\n",
      "weighted avg       0.58      0.58      0.58       403\n",
      "\n",
      "SVC Training Accuracy: 0.5039772727272728\n",
      "SVC Testing Accuracy: 0.5012406947890818\n",
      "\n",
      "SVC Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.51      0.47       177\n",
      "           1       0.56      0.50      0.53       226\n",
      "\n",
      "    accuracy                           0.50       403\n",
      "   macro avg       0.50      0.50      0.50       403\n",
      "weighted avg       0.51      0.50      0.50       403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vectorbt as vbt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 1. Data Preparation\n",
    "close_data = data[['cad_ig_er_index', 'us_ig_er_index', 'us_hy_er_index', 'risk_free_index']]\n",
    "close_data.index = pd.to_datetime(close_data.index)\n",
    "close_data = close_data.sort_index()\n",
    "\n",
    "# Feature Engineering: Adding volatility\n",
    "lookback_vol = 20\n",
    "close_data['us_ig_vol'] = close_data['us_ig_er_index'].pct_change().rolling(lookback_vol).std()\n",
    "close_data['us_hy_vol'] = close_data['us_hy_er_index'].pct_change().rolling(lookback_vol).std()\n",
    "close_data = close_data.dropna()\n",
    "\n",
    "# Creating the feature set and target variable\n",
    "X = close_data[['us_ig_er_index', 'us_hy_er_index', 'us_ig_vol', 'us_hy_vol']]\n",
    "y = (close_data['cad_ig_er_index'].pct_change(1).shift(-1) > 0).astype(int)  # Binary target for next day return\n",
    "y = y.dropna()\n",
    "X = X.loc[y.index]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Best parameters for Random Forest\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "y_pred_train_rf = best_rf.predict(X_train_res)\n",
    "y_pred_test_rf = best_rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Training Accuracy:\", best_rf.score(X_train_res, y_train_res))\n",
    "print(\"Random Forest Testing Accuracy:\", best_rf.score(X_test, y_test))\n",
    "print(\"\\nRandom Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test_rf))\n",
    "\n",
    "# Experiment with other models\n",
    "# Gradient Boosting\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "grid_search_gb = GridSearchCV(gb, param_grid_gb, cv=5, scoring='accuracy')\n",
    "grid_search_gb.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Best parameters for Gradient Boosting\n",
    "best_gb = grid_search_gb.best_estimator_\n",
    "\n",
    "# Evaluate the Gradient Boosting model\n",
    "y_pred_train_gb = best_gb.predict(X_train_res)\n",
    "y_pred_test_gb = best_gb.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boosting Training Accuracy:\", best_gb.score(X_train_res, y_train_res))\n",
    "print(\"Gradient Boosting Testing Accuracy:\", best_gb.score(X_test, y_test))\n",
    "print(\"\\nGradient Boosting Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test_gb))\n",
    "\n",
    "# Support Vector Machine\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "svc = SVC(random_state=42)\n",
    "grid_search_svc = GridSearchCV(svc, param_grid_svc, cv=5, scoring='accuracy')\n",
    "grid_search_svc.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Best parameters for SVC\n",
    "best_svc = grid_search_svc.best_estimator_\n",
    "\n",
    "# Evaluate the SVC model\n",
    "y_pred_train_svc = best_svc.predict(X_train_res)\n",
    "y_pred_test_svc = best_svc.predict(X_test)\n",
    "\n",
    "print(\"SVC Training Accuracy:\", best_svc.score(X_train_res, y_train_res))\n",
    "print(\"SVC Testing Accuracy:\", best_svc.score(X_test, y_test))\n",
    "print(\"\\nSVC Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test_svc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Analysis\n",
    "\n",
    "#### Random Forest\n",
    "- **Training Accuracy:** 94.26%\n",
    "  - The Random Forest model performs very well on the training data, correctly predicting the target variable 94.26% of the time.\n",
    "- **Testing Accuracy:** 57.32%\n",
    "  - This lower accuracy on the test data suggests that the model might be overfitting, capturing noise in the training data that doesn't generalize well to new, unseen data.\n",
    "\n",
    "**Classification Report:**\n",
    "- **Class 0 (Not Investing):**\n",
    "  - **Precision:** 0.51 (51% of the predicted non-investing signals are correct)\n",
    "  - **Recall:** 0.55 (55% of actual non-investing signals are correctly identified)\n",
    "  - **F1-score:** 0.53 (harmonic mean of precision and recall)\n",
    "- **Class 1 (Investing):**\n",
    "  - **Precision:** 0.63 (63% of the predicted investing signals are correct)\n",
    "  - **Recall:** 0.59 (59% of actual investing signals are correctly identified)\n",
    "  - **F1-score:** 0.61\n",
    "\n",
    "#### Gradient Boosting\n",
    "- **Training Accuracy:** 99.83%\n",
    "  - The Gradient Boosting model performs extremely well on the training data, which is an indicator of potential overfitting.\n",
    "- **Testing Accuracy:** 57.82%\n",
    "  - Slightly better than Random Forest on test data but still indicates overfitting.\n",
    "\n",
    "**Classification Report:**\n",
    "- **Class 0 (Not Investing):**\n",
    "  - **Precision:** 0.52 (52% of the predicted non-investing signals are correct)\n",
    "  - **Recall:** 0.55 (55% of actual non-investing signals are correctly identified)\n",
    "  - **F1-score:** 0.54\n",
    "- **Class 1 (Investing):**\n",
    "  - **Precision:** 0.63 (63% of the predicted investing signals are correct)\n",
    "  - **Recall:** 0.60 (60% of actual investing signals are correctly identified)\n",
    "  - **F1-score:** 0.61\n",
    "\n",
    "#### Support Vector Classifier (SVC)\n",
    "- **Training Accuracy:** 50.40%\n",
    "  - The SVC model struggles to learn from the training data, likely due to the imbalance in the dataset or inappropriate hyperparameters.\n",
    "- **Testing Accuracy:** 50.12%\n",
    "  - The SVC model performs slightly better than random guessing on the test data.\n",
    "\n",
    "**Classification Report:**\n",
    "- **Class 0 (Not Investing):**\n",
    "  - **Precision:** 0.44 (44% of the predicted non-investing signals are correct)\n",
    "  - **Recall:** 0.51 (51% of actual non-investing signals are correctly identified)\n",
    "  - **F1-score:** 0.47\n",
    "- **Class 1 (Investing):**\n",
    "  - **Precision:** 0.56 (56% of the predicted investing signals are correct)\n",
    "  - **Recall:** 0.50 (50% of actual investing signals are correctly identified)\n",
    "  - **F1-score:** 0.53\n",
    "\n",
    "\n",
    "#### Analysis\n",
    "Overfitting: Both the Random Forest and Gradient Boosting classifiers exhibit high training accuracy but much lower testing accuracy, indicating overfitting. The models are capturing noise in the training data that doesn’t generalize well.\n",
    "Performance: The testing accuracies for all models are relatively low (around 50-58%), indicating that the models are not performing well on unseen data. This could be due to:\n",
    "Imbalanced classes\n",
    "Insufficient or inappropriate features\n",
    "Need for better hyperparameter tuning\n",
    "Model Selection: Given the performance, further steps can include:\n",
    "Balancing the dataset more effectively\n",
    "Feature engineering to add more relevant features\n",
    "Trying different models or ensemble methods\n",
    "Fine-tuning hyperparameters with more extensive grid search or random search\n",
    "\n",
    "\n",
    "#### Next Steps\n",
    "Balance the Dataset: Explore more advanced techniques for balancing the dataset, such as ensemble methods specifically designed for imbalanced data.\n",
    "Feature Engineering: Add more features that might capture the underlying patterns better, such as moving averages, other technical indicators, etc.\n",
    "Hyperparameter Tuning: Use more sophisticated hyperparameter tuning methods like Random Search or Bayesian Optimization.\n",
    "Model Evaluation: Implement cross-validation to better evaluate model performance and ensure that it generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Random Forest Training Accuracy: 0.9460815047021943\n",
      "Balanced Random Forest Testing Accuracy: 0.9448621553884712\n",
      "\n",
      "Balanced Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.95      0.97       379\n",
      "        True       0.47      0.90      0.62        20\n",
      "\n",
      "    accuracy                           0.94       399\n",
      "   macro avg       0.73      0.92      0.80       399\n",
      "weighted avg       0.97      0.94      0.95       399\n",
      "\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found for Random Forest: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 20}\n",
      "Best cross-validation score for Random Forest: 0.9868338557993731\n",
      "Cross-validation scores: [0.98746082 0.98432602 0.99059561 0.98119122 0.99059561]\n",
      "Mean cross-validation score: 0.9868338557993731\n",
      "Random Forest Training Accuracy: 1.0\n",
      "Random Forest Testing Accuracy: 0.9774436090225563\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.99      0.99       379\n",
      "        True       0.82      0.70      0.76        20\n",
      "\n",
      "    accuracy                           0.98       399\n",
      "   macro avg       0.90      0.85      0.87       399\n",
      "weighted avg       0.98      0.98      0.98       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vectorbt as vbt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Assuming close_data and signal are already defined\n",
    "# Add the moving average features\n",
    "close_data['us_ig_ma20'] = close_data['us_ig_er_index'].rolling(window=20).mean()\n",
    "close_data['us_hy_ma20'] = close_data['us_hy_er_index'].rolling(window=20).mean()\n",
    "\n",
    "# Drop NaN values and align X and y\n",
    "X = close_data[['us_ig_er_index', 'us_hy_er_index', 'us_ig_ma20', 'us_hy_ma20']].dropna()\n",
    "y = signal.loc[X.index]\n",
    "\n",
    "# Ensure X and y have the same index\n",
    "X = X.loc[y.index]\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Balance the Dataset using Balanced Random Forest Classifier\n",
    "brf = BalancedRandomForestClassifier(n_estimators=100, random_state=42)\n",
    "brf.fit(X_train, y_train)\n",
    "\n",
    "brf_pred_train = brf.predict(X_train)\n",
    "brf_pred_test = brf.predict(X_test)\n",
    "\n",
    "print(\"Balanced Random Forest Training Accuracy:\", brf.score(X_train, y_train))\n",
    "print(\"Balanced Random Forest Testing Accuracy:\", brf.score(X_test, y_test))\n",
    "\n",
    "print(\"\\nBalanced Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, brf_pred_test))\n",
    "\n",
    "# Hyperparameter Tuning using Random Search\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_distributions=param_dist, n_iter=100, cv=5, verbose=2, n_jobs=-1)\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best cross-validation score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Model Evaluation using Cross-Validation\n",
    "rf = RandomForestClassifier(**random_search_rf.best_params_)\n",
    "cv_scores = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean cross-validation score: {cv_scores.mean()}\")\n",
    "\n",
    "# Retrain the model on the entire training set\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred_train = rf.predict(X_train)\n",
    "rf_pred_test = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Training Accuracy:\", rf.score(X_train, y_train))\n",
    "print(\"Random Forest Testing Accuracy:\", rf.score(X_test, y_test))\n",
    "\n",
    "print(\"\\nRandom Forest Classification Report:\")\n",
    "print(classification_report(y_test, rf_pred_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Explanation of Results and Suggestions for Further Steps\n",
    "\n",
    "## Balanced Random Forest Results\n",
    "\n",
    "### Training Accuracy: 0.946\n",
    "- The model performs very well on the training data with an accuracy of 94.6%.\n",
    "\n",
    "### Testing Accuracy: 0.945\n",
    "- The model generalizes well to the test data, achieving a high accuracy of 94.5%.\n",
    "\n",
    "### Classification Report\n",
    "\n",
    "#### False (Class 0)\n",
    "- **Precision:** 0.99 - When the model predicts False, it is correct 99% of the time.\n",
    "- **Recall:** 0.95 - The model correctly identifies 95% of all False instances.\n",
    "- **F1-Score:** 0.97 - The harmonic mean of precision and recall, indicating excellent performance for False predictions.\n",
    "\n",
    "#### True (Class 1)\n",
    "- **Precision:** 0.47 - When the model predicts True, it is correct 47% of the time.\n",
    "- **Recall:** 0.90 - The model correctly identifies 90% of all True instances.\n",
    "- **F1-Score:** 0.62 - The harmonic mean of precision and recall, showing that while the model often catches True instances, it also has many false positives.\n",
    "\n",
    "### Overall\n",
    "- **Accuracy:** 0.94 - The overall accuracy across both classes is 94%.\n",
    "- **Macro Avg:** 0.73 precision, 0.92 recall, 0.80 f1-score - These are unweighted averages, providing an overall measure of performance.\n",
    "- **Weighted Avg:** 0.97 precision, 0.94 recall, 0.95 f1-score - These are weighted averages, taking into account the imbalance in the dataset.\n",
    "\n",
    "## Random Forest Results\n",
    "\n",
    "### Best Parameters from Random Search\n",
    "- **n_estimators:** 200\n",
    "- **min_samples_split:** 2\n",
    "- **min_samples_leaf:** 1\n",
    "- **max_depth:** 20\n",
    "\n",
    "### Cross-Validation Score: 0.987\n",
    "- The mean cross-validation score is 98.7%, indicating that the model performs very well on different subsets of the training data.\n",
    "\n",
    "### Training Accuracy: 1.0\n",
    "- The model perfectly fits the training data, with an accuracy of 100%.\n",
    "\n",
    "### Testing Accuracy: 0.977\n",
    "- The model generalizes extremely well to the test data, achieving an accuracy of 97.7%.\n",
    "\n",
    "### Classification Report\n",
    "\n",
    "#### False (Class 0)\n",
    "- **Precision:** 0.98 - When the model predicts False, it is correct 98% of the time.\n",
    "- **Recall:** 0.99 - The model correctly identifies 99% of all False instances.\n",
    "- **F1-Score:** 0.99 - The harmonic mean of precision and recall, indicating excellent performance for False predictions.\n",
    "\n",
    "#### True (Class 1)\n",
    "- **Precision:** 0.82 - When the model predicts True, it is correct 82% of the time.\n",
    "- **Recall:** 0.70 - The model correctly identifies 70% of all True instances.\n",
    "- **F1-Score:** 0.76 - The harmonic mean of precision and recall, indicating good performance but with room for improvement.\n",
    "\n",
    "### Overall\n",
    "- **Accuracy:** 0.98 - The overall accuracy across both classes is 97.7%.\n",
    "- **Macro Avg:** 0.90 precision, 0.85 recall, 0.87 f1-score - These are unweighted averages, providing an overall measure of performance.\n",
    "- **Weighted Avg:** 0.98 precision, 0.98 recall, 0.98 f1-score - These are weighted averages, taking into account the imbalance in the dataset.\n",
    "\n",
    "## Analysis and Next Steps\n",
    "\n",
    "### Imbalance Handling\n",
    "- The Balanced Random Forest classifier improves recall for the minority class (True), but precision is relatively low. This suggests the model is identifying most True instances but also generating false positives.\n",
    "- Consider further techniques like SMOTE for oversampling the minority class or using ensemble methods that handle imbalance more effectively.\n",
    "\n",
    "### Feature Engineering\n",
    "- Explore adding more features, such as other technical indicators (e.g., RSI, MACD) or fundamental indicators (e.g., financial ratios).\n",
    "- Feature selection techniques can help identify the most important features.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "- The Random Search has provided good results. For further improvement, consider using Bayesian Optimization, which can be more efficient and provide better hyperparameter values.\n",
    "- Explore other models like XGBoost, LightGBM, or CatBoost, which often perform well with tabular data.\n",
    "\n",
    "### Model Evaluation\n",
    "- Implement cross-validation more extensively to ensure the model generalizes well.\n",
    "- Evaluate the model's performance over different time periods to ensure stability and robustness.\n",
    "\n",
    "- **Evaluate feature importance** to understand which features contribute most to the model's predictions. This can guide further feature engineering and selection.\n",
    "\n",
    "### Implementation of Advanced Techniques\n",
    "- **Model Ensembling**: Combine predictions from multiple models to improve overall performance. Techniques like stacking, bagging, and boosting can be useful.\n",
    "- **Anomaly Detection**: Identify and treat outliers in the dataset, which can significantly impact model performance.\n",
    "- **Temporal Cross-Validation**: Since financial data is time-series based, use techniques like walk-forward validation to ensure the model is robust over different time periods.\n",
    "\n",
    "### Monitoring and Maintenance\n",
    "- **Performance Monitoring**: Continuously monitor the model’s performance in a live environment to detect any degradation over time.\n",
    "- **Periodic Retraining**: Financial markets change, so regularly retrain the model with new data to maintain performance.\n",
    "- **Backtesting**: Simulate the model’s performance on historical data to understand potential returns and risks.\n",
    "\n",
    "### Conclusion\n",
    "Both the Balanced Random Forest and the tuned Random Forest models show promising results, with the latter achieving slightly better performance metrics. However, each has its strengths and weaknesses:\n",
    "- The Balanced Random Forest is better at detecting True instances but generates more false positives.\n",
    "- The tuned Random Forest offers higher precision and overall accuracy but might overfit the training data.\n",
    "\n",
    "Combining these approaches and further refining the models through advanced techniques and thorough evaluation will help in building a robust predictive model for financial data.\n",
    "\n",
    "### Next Steps\n",
    "1. **Imbalance Handling**: Experiment with different techniques like SMOTE, ADASYN, or ensemble methods specifically designed for imbalanced data.\n",
    "2. **Feature Engineering**: Add and test new features, and use techniques like PCA or Lasso for feature selection.\n",
    "3. **Hyperparameter Tuning**: Use Bayesian Optimization or Genetic Algorithms for more efficient hyperparameter tuning.\n",
    "4. **Model Evaluation**: Implement temporal cross-validation and conduct extensive backtesting to ensure model stability.\n",
    "5. **Model Ensembling**: Explore stacking, bagging, and boosting to combine multiple models for improved performance.\n",
    "6. **Deployment**: Develop a framework for continuous monitoring and periodic retraining to adapt to new data and market conditions.\n",
    "\n",
    "By following these steps, you can build a highly accurate and robust predictive model suitable for financial trading strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: OrderedDict([('colsample_bytree', 0.1), ('learning_rate', 0.01), ('max_depth', 30), ('n_estimators', 500), ('subsample', 0.6691555385512675)])\n",
      "Best cross-validation score: 0.9574103323228602\n",
      "Cross-validation scores: [0.92420213 0.99201065 0.93608522 0.94806924 0.98668442]\n",
      "Mean cross-validation score: 0.9574103323228602\n",
      "\n",
      "XGBoost Model Performance:\n",
      "Training Accuracy: 0.9946737683089214\n",
      "Testing Accuracy: 0.9813829787234043\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.97      0.98       397\n",
      "        True       0.96      1.00      0.98       355\n",
      "\n",
      "    accuracy                           0.98       752\n",
      "   macro avg       0.98      0.98      0.98       752\n",
      "weighted avg       0.98      0.98      0.98       752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import xgboost as xgb\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Assuming close_data and signal are already defined\n",
    "\n",
    "# Step 1: Calculate volatility\n",
    "close_data['volatility_ig'] = close_data['us_ig_er_index'].pct_change().rolling(window=20).std()\n",
    "close_data['volatility_hy'] = close_data['us_hy_er_index'].pct_change().rolling(window=20).std()\n",
    "\n",
    "# Drop NaN values that result from rolling calculations\n",
    "close_data = close_data.dropna()\n",
    "\n",
    "# Ensure 'cad_ig_er_index' is included in the features DataFrame\n",
    "X = close_data[['us_ig_er_index', 'us_hy_er_index', 'volatility_ig', 'volatility_hy', 'cad_ig_er_index']]\n",
    "y = signal.loc[close_data.index]  # Ensure the signal matches the filtered data\n",
    "\n",
    "# Step 2: Imbalance Handling with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# Step 3: Feature Engineering\n",
    "def add_features(data):\n",
    "    # RSI calculation\n",
    "    delta = data['cad_ig_er_index'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    data['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD calculation\n",
    "    ema_12 = data['cad_ig_er_index'].ewm(span=12, adjust=False).mean()\n",
    "    ema_26 = data['cad_ig_er_index'].ewm(span=26, adjust=False).mean()\n",
    "    data['macd'] = ema_12 - ema_26\n",
    "    data['macd_signal'] = data['macd'].ewm(span=9, adjust=False).mean()\n",
    "    data['macd_diff'] = data['macd'] - data['macd_signal']\n",
    "    \n",
    "    # SMA calculation\n",
    "    data['sma'] = data['cad_ig_er_index'].rolling(window=20).mean()\n",
    "    \n",
    "    return data\n",
    "\n",
    "X_res = add_features(X_res)\n",
    "\n",
    "# Step 4: Hyperparameter Tuning with Bayesian Optimization\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    search_spaces={\n",
    "        'learning_rate': Real(0.01, 1.0, prior='log-uniform'),\n",
    "        'max_depth': Integer(1, 30),\n",
    "        'n_estimators': Integer(50, 500),\n",
    "        'subsample': Real(0.1, 1.0, prior='uniform'),\n",
    "        'colsample_bytree': Real(0.1, 1.0, prior='uniform'),\n",
    "    },\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit Bayesian Optimization\n",
    "bayes_search.fit(X_res, y_res)\n",
    "\n",
    "# Best parameters\n",
    "print(f\"Best parameters found: {bayes_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {bayes_search.best_score_}\")\n",
    "\n",
    "# Step 5: Model Evaluation with Cross-Validation\n",
    "best_xgb_model = bayes_search.best_estimator_\n",
    "cross_val_scores = cross_val_score(best_xgb_model, X_res, y_res, cv=5)\n",
    "print(f\"Cross-validation scores: {cross_val_scores}\")\n",
    "print(f\"Mean cross-validation score: {cross_val_scores.mean()}\")\n",
    "\n",
    "# Final model fitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Model performance on test set\n",
    "y_pred = best_xgb_model.predict(X_test)\n",
    "print(\"\\nXGBoost Model Performance:\")\n",
    "print(f\"Training Accuracy: {best_xgb_model.score(X_train, y_train)}\")\n",
    "print(f\"Testing Accuracy: {best_xgb_model.score(X_test, y_test)}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Machine Learning Strategy Portfolio Statistics:\n",
      "Start                         2002-11-29 00:00:00\n",
      "End                           2024-06-28 00:00:00\n",
      "Period                         2033 days 00:00:00\n",
      "Start Value                              100000.0\n",
      "End Value                           122698.035069\n",
      "Total Return [%]                        22.698035\n",
      "Benchmark Return [%]                    37.286799\n",
      "Max Gross Exposure [%]                      100.0\n",
      "Total Fees Paid                               0.0\n",
      "Max Drawdown [%]                        10.598642\n",
      "Max Drawdown Duration           296 days 00:00:00\n",
      "Total Trades                                    1\n",
      "Total Closed Trades                             0\n",
      "Total Open Trades                               1\n",
      "Open Trade PnL                       22698.035069\n",
      "Win Rate [%]                                  NaN\n",
      "Best Trade [%]                                NaN\n",
      "Worst Trade [%]                               NaN\n",
      "Avg Winning Trade [%]                         NaN\n",
      "Avg Losing Trade [%]                          NaN\n",
      "Avg Winning Trade Duration                    NaT\n",
      "Avg Losing Trade Duration                     NaT\n",
      "Profit Factor                                 NaN\n",
      "Expectancy                                    NaN\n",
      "Sharpe Ratio                             0.890953\n",
      "Calmar Ratio                             0.352953\n",
      "Omega Ratio                              1.185881\n",
      "Sortino Ratio                            1.288519\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Now proceed with backtesting using vectorbt\n",
    "\n",
    "# 1. Signal Generation using trained model\n",
    "X_test_orig = add_features(X)  # Add features to the original dataset\n",
    "y_pred_signal = best_xgb_model.predict(X_test_orig)\n",
    "\n",
    "# Ensure the signal is boolean\n",
    "signal_ml = pd.Series(y_pred_signal, index=X_test_orig.index).astype(bool)\n",
    "\n",
    "# Reindex the signals to match the close_data index\n",
    "signal_ml = signal_ml.reindex(close_data.index, method='ffill').fillna(False)\n",
    "\n",
    "# 2. Portfolio Creation using the machine learning signal\n",
    "portfolio_ml = vbt.Portfolio.from_signals(\n",
    "    close=close_data['cad_ig_er_index'],\n",
    "    entries=signal_ml.shift(1).fillna(False),  # Enter the day after the signal\n",
    "    exits=signal_ml.shift(1).fillna(False) & (~signal_ml).fillna(False),  # Exit when signal changes from True to False\n",
    "    init_cash=100000,\n",
    "    fees=0,\n",
    "    freq='D'\n",
    ")\n",
    "\n",
    "# 3. Generate and print portfolio stats for the ML-based strategy\n",
    "try:\n",
    "    stats_ml = portfolio_ml.stats()\n",
    "    print(\"\\nMachine Learning Strategy Portfolio Statistics:\")\n",
    "    print(stats_ml)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while generating ML portfolio stats: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "home_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
